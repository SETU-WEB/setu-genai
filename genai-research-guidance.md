# genai research guidance

Generative AI in Research: Guidance for Researchers and Supervisors

## Generative AI in Research

Guidance for Researchers & Supervisors

Supporting the ethical, responsible, critical and transparent use of GenAI by researchers and supervisors

‚ÑπÔ∏è

Living Guidelines

This guidance is regularly updated to reflect the rapidly evolving GenAI landscape. Last updated: October 2025.

### Introduction

Generative AI has great potential for accelerating scientific discovery and improving research productivity. Positive applications include improved inclusivity through translation, efficient literature synthesis, and assistance with data analysis code.

However, GenAI also entails significant risks including data privacy concerns, lack of reliability and reproducibility, potential reproduction of biases, and threats to intellectual property and research integrity.

‚ö†Ô∏è

Cognitive Offloading Risk

Recent studies have found a significant negative correlation between frequent AI tool usage and critical thinking abilities. Researchers must engage critically with AI technologies and avoid over-reliance.

### Purpose

This guidance supports researchers and students engaged in research on the considerations needed to use GenAI ethically, responsibly and critically. It provides:

* Core principles aligned with EU and national research integrity frameworks
* Clear guidance on data protection and GDPR compliance
* Practical case studies addressing common scenarios
* A decision framework for determining appropriate AI usage levels
* Specific guidance for supervisors working with research students

### Scope

This guidance applies to all individuals and groups who engage in, or support, research activities. This includes:

* All permanent, part-time or contract staff engaged in research
* Those involved in setting research priorities, strategies, policies and assessment
* All enrolled students who engage in research
* Research conducted on any campus or on behalf of the institution at any location

## Core Principles

Framed around the EU Living Guidelines on the Responsible Use of Generative AI in Research

The use of GenAI in research must be underpinned by the notion of **human agency and human oversight**. Researchers remain ultimately responsible for all aspects of their scientific outputs, including accountability for the integrity, accuracy, validity, and originality of all content in their research publications.

#### üéØ Reliability

Ensuring the quality of research, reflected in the design, methodology, analysis and use of resources. This includes verifying and reproducing information produced by AI for research, and being aware of possible equality and non-discrimination issues in relation to bias and inaccuracies.

#### üîç Honesty

Developing, carrying out, reviewing, reporting and communicating on research transparently, fairly, thoroughly and impartially. This principle includes disclosing that generative AI has been used, what tools have been used, how they have been used and for what purposes.

#### ü§ù Respect

For colleagues, research participants, research subjects, society, and the environment. Responsible use of generative AI should consider the limitations of the technology, its environmental impact and its societal effects (bias, diversity, non-discrimination, fairness and prevention of harm). This includes the proper management of information, compliance with GDPR legislation, respect for privacy, confidentiality and intellectual property rights.

#### üìã Accountability

For the research from idea to publication, for its management and organisation, for training, supervision and mentoring, and for its wider societal impacts. Researchers remain ultimately responsible for all aspects of their scientific outputs, including the integrity, accuracy, validity, and originality of all content in their research publications.

### Applying Principles Across AI Usage Levels

These principles have different implications depending on the level of AI involvement in a task:

| Principle          | Lower AI Involvement (Levels 1-2) | Higher AI Involvement (Levels 4-5)                                   |
| ------------------ | --------------------------------- | -------------------------------------------------------------------- |
| **Reliability**    | Standard verification practices   | Extensive verification essential; reproducibility may be compromised |
| **Honesty**        | Brief acknowledgment may suffice  | Detailed disclosure of AI contribution required                      |
| **Respect**        | Standard data protection          | Enhanced scrutiny of data inputs; IP protection critical             |
| **Accountability** | Clear researcher ownership        | Accountability strained; may be difficult to defend work             |

‚ö†Ô∏è

Key Question

At what point does AI involvement make genuine accountability impossible? If a researcher cannot explain or defend work they have submitted, accountability is compromised regardless of disclosure.

## Risks & Limitations

Understanding the limitations of GenAI tools is essential for responsible use

GenAI tools have significant limitations that researchers must understand before using them in any aspect of their work. These are not edge cases‚Äîthey are inherent characteristics of current AI systems.

*   **‚ùå AI Gets Things Wrong**

    AI will produce incorrect outputs and does not know right from wrong. It presents all outputs as if they are equally valid and true, regardless of accuracy.
*   **‚öñÔ∏è AI is Biased**

    AI tools reflect and amplify biases and stereotypes from their training data. They cannot judge inaccurate or offensive statements, or assess validity and accuracy.
*   **üé≠ AI Makes Things Up**

    Called 'hallucinations', AI tools will fabricate false references to texts that do not exist. They can also hallucinate content, claiming a publication says something it does not.
*   **üîÑ AI is Unreliable**

    AI tools cannot access all necessary data, are often not trained on recent information, may provide different answers to the same prompt, cannot access paywalled content, and may recommend articles from predatory journals.
*   **üëç AI Provides Uncritical Responses**

    AI tools tend to provide answers that flatter and please the user. They rarely criticise inputs or tell users when their prompts have flaws. You must explicitly ask GenAI to be critical.

### Implications for Research

Literature Reviews

AI cannot replace traditional methods

AI research assistant tools can be complementary tools during systematic literature reviews, but their results lack repeatability and reliability. They should not replace traditional database searches (JSTOR, PubMed, etc.) as they cannot access paywalled content and may miss high-quality academic publications.

Data Analysis

Verification is essential

AI-generated code or statistical outputs can contain logical errors, misuse statistical tests, or make invalid assumptions. All AI-assisted analysis must be validated with traditional statistical packages or peer review. Avoid "black box" analyses where you cannot explain the process.

Writing Assistance

AI can change meaning

GenAI can change meaning and introduce errors when editing or proofreading. Additionally, AI-edited text may trigger AI detection tools, potentially leading to false accusations of misconduct. Always keep all draft versions to demonstrate your work throughout.

Research Misconduct

The unauthorised or unacknowledged use of AI-generated text, data, images, code, or ideas in a research output may be regarded as plagiarism and could be investigated under research misconduct procedures.

## Data Protection, Privacy & Confidentiality

GDPR compliance is mandatory when using GenAI tools

üö´

Critical Rule

Never input personal data, sensitive personal data, or confidential information into external GenAI tools without proper safeguards and lawful basis.

### Key Definitions

Personal Data

Any information relating to an identified or identifiable person. This includes direct identifiers (name, address, ID number) and indirect identifiers (IP address, characteristics that can lead to identification when combined).

Private Data

Information an individual expects to be kept confidential, even if it doesn't directly identify them. This includes unpublished research ideas, intellectual property, confidential interviews, internal correspondence, or sensitive commercial data from research partners.

Sensitive Personal Data (Special Category)

Data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, genetic data, biometric data, health data, or data concerning sex life or sexual orientation. This requires the highest standard of protection.

### Approved Tools

‚úÖ

Microsoft Copilot Chat

Currently the only GenAI tool with an institutional enterprise licence. Access via Office 365 Online using your institutional credentials. Your prompts and responses are protected, stored securely, and not shared outside the institution.

### What You Must NOT Input

| Data Type                                   | External GenAI | Approved Tools  |
| ------------------------------------------- | -------------- | --------------- |
| Personal data (names, contact details)      | Prohibited     | With safeguards |
| Sensitive personal data (health, ethnicity) | Prohibited     | Prohibited\*    |
| Unpublished research / thesis drafts        | Prohibited     | Caution advised |
| Grant applications                          | Prohibited     | Caution advised |
| Confidential institutional documents        | Prohibited     | Check policy    |
| Others' intellectual property               | Prohibited     | Prohibited      |

* Except with explicit consent, ethics approval, and appropriate safeguards

### If Using External Tools

If you use any GenAI tools not supported by your institution:

* Check the provider's terms and conditions regarding training models and use of inputted data
* Understand that providers may retain information for model training purposes
* Be aware that your inputs may be subject to Freedom of Information requests
* Treat AI/GenAI as a public forum‚Äîassume anything you input may be shared
* Complete GDPR training (available via Legal Island, repeat every two years)

### Unlawful Outputs

Researchers must also be mindful of unlawful outputs that GenAI can produce. These tools may have been trained on protected personal data, copyright-protected content, intellectual property, misinformation, hate speech, and other unlawful content. As a result, outputs may contain such material. Human oversight of all outputs is essential.

## Acknowledgment & Citation

Transparency in GenAI use is essential for research integrity

The Code of Conduct for Responsible Research Practice requires researchers to be transparent and acknowledge their use of AI tools and Generative AI in their research. **Failure to do so may constitute research misconduct.**

### When to Acknowledge

Any use of AI in the research process should be acknowledged, including but not limited to:

* Literature search and review
* Methods development
* Data collection assistance
* Data analysis and coding
* Writing assistance
* Translation
* Transcription
* Image generation

üí°

Practical Approach

The level of acknowledgment should match the level of AI involvement. Brief assistance (Level 2) may need only a simple acknowledgment; substantial AI contribution (Level 4) requires detailed documentation.

### Where to Acknowledge

An acknowledgment typically appears in:

* The methods section (for methodological AI use)
* The acknowledgments section at the end of a publication
* Appendices (for detailed prompts and outputs, where appropriate)

### How to Cite GenAI

**AI tools cannot be listed as authors.** This is the stance of most academic publishers because AI cannot produce original ideas nor take responsibility for or explain outputs.

The current consensus is to treat AI use as private correspondence. General rules:

* Name the AI platform used (e.g., OpenAI ChatGPT, Anthropic Claude)
* Include details on the date of use
* Ideally include details on the prompts input
* Include details of the person who input the prompts
* Keep records of the responses output by AI
* Do not present any responses from AI as your own

### Publisher and Funder Requirements

Always check the specific guidelines of publishers (e.g., Springer, Elsevier, Taylor & Francis) and funding bodies before using AI. These organisations differ in what they allow regarding:

* AI use in peer review
* Citing AI as an author (generally prohibited)
* AI for copy editing and improving language
* AI in images and figures

### Documentation Best Practice

Researchers are encouraged to document and save all prompts and associated outputs throughout their research. This is important in case there are ever any concerns around the originality of a publication or dissertation.

üìÅ

Keep Draft Versions

If using AI for proofreading or editing, always keep all different draft versions of your work. This protects you against false accusations from AI detection tools and demonstrates your work throughout the writing process.

## Guidance for Supervisors

Supporting responsible AI use in research supervision

### Supervisor Responsibilities

Supervisors (and students) have a responsibility to:

* Inform and familiarise themselves with GenAI tools and their use in research
* Understand the ethical considerations and limitations
* Keep up to date with institutional guidelines
* Know what publishers and funding agencies consider appropriate usage
* Direct students to relevant guidance on responsible research practices
* Reiterate that students remain responsible for their own work
* Ensure students can verify and explain all aspects of their research

### AI Supervisory Agreement

üìã

Recommended Practice

Establish clear expectations from the start

Consider establishing an "AI Supervisory Team and Student Agreement" at the start of the research project. This should:

* Consider GDPR and ethics requirements
* Specify which AI tools may be used and for what purposes
* Define acceptable vs. unacceptable uses for this specific project
* Be revisited annually during the progress monitoring process
* Evolve as the student's skills develop

This collaborative approach avoids positioning the supervisor as a "police officer" and instead fosters open dialogue about AI use.

### AI Detection Tools

‚ö†Ô∏è

Contested Accuracy

GenAI technologies are evolving faster than detection tools. Supervisors should be aware of the contested nature of AI detection. Turnitin's AI detection can be a useful flag to start a conversation, but should not be viewed as evidence of misconduct. Use professional judgment and consider it a prompt for deeper discussion.

### Discussion Points with Students

Supervisors should have open conversations with students about:

At Project Start

* What GenAI tools are you familiar with?
* What tools might be useful for this specific project?
* What are the limitations and risks?
* What level of AI use is appropriate for different tasks?
* How will we document AI use?

During Supervision

* How are you using AI tools in your current work?
* Can you explain and defend all aspects of your analysis?
* Have you verified AI outputs against original sources?
* Are you developing the skills you need, or relying on AI?

### Skill Development Considerations

The PhD is both a research project and a training programme. Supervisors should consider:

* Which skills must the researcher develop independently?
* What competencies will future employers expect?
* Would AI assistance on this task prevent important learning?
* Should the student first complete a task without AI to develop skills, then use AI later to extend their work?

üìö

Literature Review Example

Consider asking students to first prepare a literature review without AI tools to develop necessary search, synthesis and critical skills. AI tools can be introduced later to expand on the literature review chapter once foundational skills are established.

## Case Studies

Practical examples with ethical checklists

#### Case Study 1: External GenAI Transcription Tools

A postgraduate research student wants to use an external GenAI tool (e.g., Otter.ai, Whisper) to transcribe focus group interviews containing personal but not sensitive data. The GenAI tool is not university-approved.

**Framework Assessment: Level 2-4 depending on verification**

**Ethical Checklist**

* **Prefer approved tools** ‚Äì Use MS Teams transcription which keeps data inside the university's secure environment.
* **If using external tools:** Check provider's ownership, location, and data storage. Prioritise EU-based tools (more likely to be GDPR-compliant).
* **Confirm data privacy terms** ‚Äì Check whether the tool uses uploaded content for AI training.
* **Ensure encryption** ‚Äì Audio files should be encrypted during upload and stored securely.
* **Update consent forms** ‚Äì Include tool name, provider, data type being processed, storage location, and GDPR compliance measures.
* **Conduct risk assessment** ‚Äì Submit completed assessment with updated consent forms to Research Ethics Committee.
* **Minimise disclosure** ‚Äì Only share necessary personal data with third parties.

#### Case Study 2: GenAI for Literature Reviews

A PhD student is considering using GenAI research assistant tools (e.g., Elicit, Consensus, SciSpace, Research Rabbit, Semantic Scholar, Notebook LM) to assist with their literature review. They ask for advice on appropriateness and ethical concerns.

**Framework Assessment: Level 2-3 for search assistance; Level 4-5 problematic for synthesis**

**Ethical Checklist**

* **Understand database limits** ‚Äì Most tools rely on open-access sources and won't access paywalled literature. Use alongside traditional databases (JSTOR, PubMed).
* **Verify all outputs** ‚Äì Summaries may omit critical information, include hallucinations, contain fabricated references, or show bias.
* **ALWAYS read full original papers** ‚Äì Never rely solely on AI summaries.
* **Develop skills first** ‚Äì Consider preparing initial literature review without AI to develop search, synthesis and critical skills.
* **Agree boundaries with supervisor** ‚Äì Establish acceptable vs. unacceptable uses early.
* **Be aware of bias** ‚Äì AI searches may favour certain disciplines, regions, or publication types.
* **Acknowledge usage** ‚Äì Record AI tool use in research notes and disclose in methodology.
* **Protect your work** ‚Äì Do not upload large portions of your writing into external AI tools.

#### Case Study 3: Proofreading with GenAI

A researcher working on a co-authored research article wants to use a GenAI tool to help with proofreading and language refinement before submission. They ask whether this is acceptable, what risks are involved, and whether AI use needs to be disclosed.

**Framework Assessment: Level 2-3 (generally acceptable with precautions)**

**Ethical Checklist**

* **Check journal guidelines** ‚Äì Publishers vary in what they allow.
* **Seek co-author consent** ‚Äì All authors should agree to AI use.
* **Check funder guidelines** ‚Äì Some funders have specific requirements.
* **Review institutional rules** ‚Äì Ensure compliance with university policies.
* **Consider confidentiality** ‚Äì Check terms and conditions before uploading material.
* **Verify changes carefully** ‚Äì AI can change meaning and introduce errors.
* **Keep all draft versions** ‚Äì Protects against false AI detection accusations.
* **Acknowledge use** ‚Äì Even if journal doesn't require it.
* **Use approved tools** ‚Äì Copilot is safest for protecting IP.

#### Case Study 4: GenAI for Data Analysis/Coding

A PhD student is working with a large dataset and wants to use a GenAI tool (e.g., ChatGPT, Claude, GitHub Copilot) to clean data, run statistical analyses, or write code. The dataset contains confidential or potentially identifiable information.

**Framework Assessment: Level 3-4 (requires careful consideration)**

**Ethical Checklist**

* **Data protection first** ‚Äì Do not upload identifiable or confidential data into external AI tools.
* **Remove identifiers** ‚Äì Strip all personal identifiers and sensitive variables before use.
* **Check data policies** ‚Äì Confirm whether the tool stores, reuses, or trains on uploaded data.
* **Ensure reproducibility** ‚Äì Outputs must be independently verifiable without relying solely on AI.
* **Avoid black box analysis** ‚Äì You must be able to explain the process.
* **Document everything** ‚Äì Record all AI-assisted steps including prompts, parameters, and human verification.
* **Validate results** ‚Äì Check against traditional statistical packages or peer review.
* **Use AI to assist, not replace** ‚Äì Supervisors may require manual replication of key steps.
* **Consider using synthetic data** ‚Äì Develop and test code on synthetic data before applying to real data.

## AI Usage Level Framework

A 5-level rubric for evaluating appropriate AI involvement in research tasks

This framework provides a structured approach for research teams to discuss and establish appropriate AI use boundaries. What is appropriate varies by task, discipline, and context‚Äîthe framework supports decision-making rather than prescribing universal rules.

{% stepper %}
{% step %}
### No AI Usage

The researcher completes the task entirely independently without any AI assistance. Appropriate for tasks where unassisted human performance is essential to the integrity of the research or the development of the researcher.
{% endstep %}

{% step %}
### Light Assistance

AI serves as a reference tool or provides minor support. The researcher maintains full intellectual ownership, using AI only for explanations, clarifications, or checking work. Comparable to consulting a textbook.
{% endstep %}

{% step %}
### Moderate Collaboration

AI provides substantive suggestions or content that the researcher evaluates, adapts, and integrates. The researcher remains the decision-maker and author, but AI contributions meaningfully shape the output. Comparable to detailed feedback from a colleague.
{% endstep %}

{% step %}
### Substantial Delegation

AI produces significant portions of the work product, which the researcher reviews and verifies. The researcher's role shifts toward direction, curation, and quality assurance. Intellectual ownership becomes shared or ambiguous.
{% endstep %}

{% step %}
### Full Offload

AI completes the task with minimal human input, verification, or intellectual engagement. The researcher accepts AI outputs without substantive evaluation. This level raises significant questions about authorship, learning, and research integrity.
{% endstep %}
{% endstepper %}

### Implications by Level

| Level | Verification     | Disclosure                 | Accountability | Appropriateness        |
| ----- | ---------------- | -------------------------- | -------------- | ---------------------- |
| **1** | N/A              | None needed                | Full           | Generally appropriate  |
| **2** | Standard         | Brief if any               | Full           | Generally appropriate  |
| **3** | Thorough         | Clear acknowledgment       | Maintained     | Context dependent      |
| **4** | Extensive        | Detailed documentation     | Strained       | Requires justification |
| **5** | Often impossible | Essential but insufficient | Questionable   | Rarely appropriate     |

### Key Questions for Decision-Making

About the Task

* Is this task central to the doctoral contribution?
* Does it require skills the researcher must develop?
* Would AI assistance affect the validity of the research?
* Are there accuracy risks if AI makes errors?

About Transparency

* How will AI use be documented and disclosed?
* What would examiners, reviewers, or employers expect?
* Does the researcher understand what the AI produced?
* Can they explain and defend all aspects of the work?

About Learning

* What is the researcher missing by not doing this themselves?
* Will they need this skill in future roles?
* Is struggling with this task part of intellectual development?
* Could AI assistance prevent deeper understanding?

## Task-Level Guidance

AI usage examples across the research lifecycle

This section provides specific examples of AI usage at each level for common research tasks. Use this as a discussion tool with supervisors to establish appropriate boundaries for your project.

#### 1 Orientation & Scoping

‚ñº

Conducting preliminary literature searches

{% stepper %}
{% step %}
Conduct all searches manually using databases and citation tracking
{% endstep %}

{% step %}
Using AI to suggest search terms or databases to try
{% endstep %}

{% step %}
Asking AI to identify key authors, papers, or debates in an area
{% endstep %}

{% step %}
Having AI produce annotated reading lists with summaries
{% endstep %}

{% step %}
Asking AI to produce a comprehensive field map with no human searching
{% endstep %}
{% endstepper %}

Formulating research questions

{% stepper %}
{% step %}
Develop questions through researcher reflection and supervisor discussion
{% endstep %}

{% step %}
Using AI to check whether questions are clearly worded and scoped
{% endstep %}

{% step %}
Asking AI to suggest refinements or sub-questions
{% endstep %}

{% step %}
Having AI generate alternative formulations of the research question
{% endstep %}

{% step %}
Asking AI to propose research questions based on identified gaps
{% endstep %}
{% endstepper %}

Developing the theoretical framework

{% stepper %}
{% step %}
Develop framework through independent reading and intellectual work
{% endstep %}

{% step %}
Using AI to explain theoretical concepts or compare frameworks
{% endstep %}

{% step %}
Asking AI to suggest which theories might be applicable
{% endstep %}

{% step %}
Having AI draft a conceptual framework showing concept relationships
{% endstep %}

{% step %}
Asking AI to select and fully articulate the theoretical framework
{% endstep %}
{% endstepper %}

#### 2 Research Design

‚ñº

Designing data collection instruments

{% stepper %}
{% step %}
Design instruments entirely based on methodological expertise
{% endstep %}

{% step %}
Using AI to check instrument wording for clarity or bias
{% endstep %}

{% step %}
Asking AI to suggest questions used in validated instruments
{% endstep %}

{% step %}
Having AI draft complete instruments based on research questions
{% endstep %}

{% step %}
Using AI-generated instruments without refinement or piloting
{% endstep %}
{% endstepper %}

Completing ethics applications

{% stepper %}
{% step %}
Complete forms entirely based on own understanding
{% endstep %}

{% step %}
Using AI to clarify what form questions are asking
{% endstep %}

{% step %}
Asking AI to suggest how to articulate ethical safeguards
{% endstep %}

{% step %}
Having AI draft responses to ethics application questions
{% endstep %}

{% step %}
Submitting AI-drafted applications without researcher verification
{% endstep %}
{% endstepper %}

#### 3 Data Collection & Management

‚ñº

Transcribing recordings

{% stepper %}
{% step %}
Transcribe all recordings manually
{% endstep %}

{% step %}
Using AI transcription with human verification of accuracy
{% endstep %}

{% step %}
Using AI transcription with spot-checking
{% endstep %}

{% step %}
Using AI transcription with minimal review
{% endstep %}

{% step %}
Using AI transcription without any verification
{% endstep %}
{% endstepper %}

Organising and structuring data

{% stepper %}
{% step %}
Develop data structure entirely through own analysis
{% endstep %}

{% step %}
Using AI to explain best practices for data organisation
{% endstep %}

{% step %}
Having iterative conversation with AI to develop structure, researcher makes final decision
{% endstep %}

{% step %}
Having AI propose complete data structure based on examples
{% endstep %}

{% step %}
Implementing AI structure without researcher evaluation
{% endstep %}
{% endstepper %}

#### 4 Analysis & Interpretation

‚ñº

Coding qualitative data

{% stepper %}
{% step %}
Code all data manually through close reading
{% endstep %}

{% step %}
Using AI to check coding consistency
{% endstep %}

{% step %}
Having AI suggest codes for difficult passages
{% endstep %}

{% step %}
Asking AI to code data with researcher reviewing a sample
{% endstep %}

{% step %}
AI coding all data without systematic human verification
{% endstep %}
{% endstepper %}

Writing analysis code

{% stepper %}
{% step %}
Write all code independently
{% endstep %}

{% step %}
Using AI to debug or explain code errors
{% endstep %}

{% step %}
Having AI suggest code improvements or approaches
{% endstep %}

{% step %}
Asking AI to write code based on researcher specifications, validated against known tools
{% endstep %}

{% step %}
Using AI-generated code without understanding or verification
{% endstep %}
{% endstepper %}

Developing interpretations

{% stepper %}
{% step %}
Develop all interpretations through researcher reflection
{% endstep %}

{% step %}
Using AI to test interpretations against evidence
{% endstep %}

{% step %}
Having AI suggest alternative interpretations
{% endstep %}

{% step %}
Asking AI to develop interpretations from data
{% endstep %}

{% step %}
AI generating interpretations without researcher intellectual engagement
{% endstep %}
{% endstepper %}

#### 5 Writing & Synthesis

‚ñº

Drafting thesis chapters

{% stepper %}
{% step %}
Write all drafts independently
{% endstep %}

{% step %}
Using AI to overcome writer's block or clarify expression
{% endstep %}

{% step %}
Having AI expand bullet points into prose
{% endstep %}

{% step %}
Asking AI to draft sections based on notes and findings
{% endstep %}

{% step %}
AI writing chapters without researcher drafting
{% endstep %}
{% endstepper %}

Proofreading and editing

{% stepper %}
{% step %}
Proofread entirely manually
{% endstep %}

{% step %}
Using AI to catch errors and typos
{% endstep %}

{% step %}
Having AI copy-edit for style and grammar
{% endstep %}

{% step %}
Asking AI to proofread and correct entire document
{% endstep %}

{% step %}
AI finalising text without human review
{% endstep %}
{% endstepper %}

#### 6 Dissemination & Defence

‚ñº

Preparing for viva examination

{% stepper %}
{% step %}
Prepare entirely through own review and human mock vivas
{% endstep %}

{% step %}
Using AI to identify likely areas of questioning
{% endstep %}

{% step %}
Having AI generate practice questions
{% endstep %}

{% step %}
Asking AI to conduct mock viva
{% endstep %}

{% step %}
Level 5 not applicable: preparation must develop researcher's own understanding for the actual examination
{% endstep %}
{% endstepper %}

Responding to peer review

{% stepper %}
{% step %}
Respond to all reviews independently
{% endstep %}

{% step %}
Using AI to understand reviewer concerns
{% endstep %}

{% step %}
Having AI suggest response strategies
{% endstep %}

{% step %}
Asking AI to draft response letter and revisions
{% endstep %}

{% step %}
AI managing peer review without researcher engagement
{% endstep %}
{% endstepper %}

## Resources & Training

Support for responsible AI use in research

### Institutional Resources

Generative AI Hubs

Moodle and Blackboard

Dedicated hubs with resources for students and staff, including information on available GenAI tools and guidance on their use.

Library AI Literacy Tutorial

Generative AI and Prompt Literacy Guide

Comprehensive guide covering AI literacy and literature search strategies for researchers.

AI Connect Community of Practice

For all researchers

Events throughout the year covering topics such as AI in publications, ethical use of AI in research, and how AI is transforming the PhD process.

CPD Week Workshops

Regular training opportunities

Regular workshops on the ethical use of AI in research delivered during CPD week.

### Required Training

GDPR Training

Researchers processing personal data must complete the Data Protection Module via Legal Island. This one-hour module should be repeated every two years.

### Key Policy Documents

* Code of Conduct for Responsible Practice of Research
* Procedures for Managing Allegations of Misconduct in Research
* Intellectual Property Policy
* Data Protection Policy
* Research Ethics Committee Operations and Application Guidance
* Academic Integrity Policy
* Student Academic Misconduct Policy and Disciplinary Procedure
* Staff and Student Guidelines on the Use of GenAI

### External References

* European Commission (2024) Artificial Intelligence Act
* European Commission (2024) Living Guidelines on the Responsible Use of Generative AI in Research
* National Academic Integrity Network (2023) Generative Artificial Intelligence: Guidelines for Educators
* UNESCO (2023) Guidance for Generative AI in Education and Research
* All European Academies (ALLEA) European Code of Conduct for Research Integrity (2023)

### Contact

If you have concerns or questions about responsible, ethical and appropriate use of GenAI in research, contact the Research Integrity and Compliance Officer.

This guidance is regularly reviewed and updated. Last updated: October 2025.
